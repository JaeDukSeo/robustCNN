function net = cnn_cifar_init_robust(safType, meanTraining)
% safType: rbf1d, or mrelu
% meanTraining: whether to augment the train set with noisy mean samples

opts.networkType = 'simplenn' ;
opts.useBatchNorm = true;
lr = [0.1 0.1];

% Define network CIFAR10-quick
net.layers = {} ;
if meanTraining, categoryN = 12; else categoryN = 10; end;
iw1 = 0.01; iw2 = 0.01; iw3 = 0.01; iw4 = 0.01; iw5 = 0.01;

% Block 1
ln1 = 32; ln2 = 32; ln3 = 64; ln4 = 64;
net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{iw1*randn(5,5,3,ln1, 'single'), zeros(1,ln1, 'single')}}, ...
    'learningRate', lr, ...
    'stride', 1, ...
    'pad', 2) ;
net.layers{end+1} = struct('type', safType) ;
net.layers{end+1} = struct('type', 'pool', ...
    'method', 'max', ...
    'pool', [3 3], ...
    'stride', 2, ...
    'pad', [0 1 0 1]) ;

% Block 2
net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{iw2*randn(5,5,ln1,ln2, 'single'), zeros(1,ln2,'single')}}, ...
    'learningRate', lr, ...
    'stride', 1, ...
    'pad', 2) ;
net.layers{end+1} = struct('type', safType) ;
net.layers{end+1} = struct('type', 'pool', ...
    'method', 'avg', ...
    'pool', [3 3], ...
    'stride', 2, ...
    'pad', [0 1 0 1]) ; % Emulate caffe

% Block 3
net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{iw3*randn(5,5,ln2,ln3, 'single'), zeros(1,ln3,'single')}}, ...
    'learningRate', lr, ...
    'stride', 1, ...
    'pad', 2) ;
net.layers{end+1} = struct('type', safType) ;
net.layers{end+1} = struct('type', 'pool', ...
    'method', 'avg', ...
    'pool', [3 3], ...
    'stride', 2, ...
    'pad', [0 1 0 1]) ; % Emulate caffe

% Block 4
net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{iw4*randn(4,4,ln3,ln4, 'single'), zeros(1,ln4,'single')}}, ...
    'learningRate', lr, ...
    'stride', 1, ...
    'pad', 0) ;
net.layers{end+1} = struct('type', 'relu') ;

% Block 5
net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{iw5*randn(1,1,ln4,categoryN, 'single'), zeros(1,categoryN,'single')}}, ...
    'learningRate', 1*lr, ...
    'stride', 1, ...
    'pad', 0) ;

% Loss layer
net.layers{end+1} = struct('type', 'rbf1d') ;
net.layers{end+1} = struct('type', 'hybridloss', 'para', [1, 1, 1/9, 2]) ;

% optionally switch to batch normalization
net = insertBnorm(net, 1) ;
net = insertBnorm(net, 5) ;
net = insertBnorm(net, 9) ;
net = insertBnorm(net, 13) ;

% Meta parameters
net.meta.inputSize = [32 32 3] ;
net.meta.trainOpts.learningRate = [0.05*ones(1,60) 0.005*ones(1,20) 0.0005*ones(1,10)] ;
net.meta.trainOpts.weightDecay = 0.0001 ;
net.meta.trainOpts.batchSize = 50 ;
net.meta.trainOpts.numEpochs = numel(net.meta.trainOpts.learningRate) ;

% Fill in default values
net = vl_simplenn_tidy(net) ;

% --------------------------------------------------------------------
function net = insertBnorm(net, l)
% --------------------------------------------------------------------
assert(isfield(net.layers{l}, 'weights'));
ndim = size(net.layers{l}.weights{1}, 4);
layer = struct('type', 'bnorm', ...
    'weights', {{ones(ndim, 1, 'single'), zeros(ndim, 1, 'single')}}, ...
    'learningRate', [1 1 0.05], ...
    'weightDecay', [0 0]) ;
net.layers{l}.biases = [] ;
net.layers = horzcat(net.layers(1:l), layer, net.layers(l+1:end)) ;
